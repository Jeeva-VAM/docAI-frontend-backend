/**
 * Chunked Upload Service
 * Handles large file uploads with progress tracking and backend API integration
 * Compatible with backend contract: init -> upload chunks -> complete
 */

export interface ChunkUploadOptions {
  chunkSize?: number;
  maxRetries?: number;
  onProgress?: (progress: ChunkUploadProgress) => void;
  onChunkComplete?: (chunkIndex: number, total_chunks: number) => void;
  onError?: (error: Error) => void;
}

export interface ChunkUploadProgress {
  fileId: string;
  fileName: string;
  fileSize: number;
  uploadedBytes: number;
  totalBytes: number;
  percentage: number;
  uploadSpeed: number; // bytes per second
  estimatedTimeRemaining: number; // seconds
  currentChunk: number;
  total_chunks: number;
  status: 'initializing' | 'uploading' | 'paused' | 'completed' | 'error' | 'cancelled';
  errorMessage?: string;
}

// Backend API types based on your contract
export interface ChunkedUploadInitRequest {
  fileName: string;
  fileSize: number;
  fileType: string;
  chunkSize: number;
  relativePath?: string | null;
}

export interface ChunkedUploadInitResponse {
  uploadId: string;
  chunkSize: number;
  totalChunks: number;
}

export interface ChunkedUploadCompleteRequest {
  fileId: string;            // Maps to uploadId for backend
  projectId: string;         // Project UUID
  folderId?: string | null;  // Folder UUID (optional)
  name: string;              // File name
  url: string;               // File URL (will be generated by backend)
  batch_id?: string;         // Batch ID for grouping uploads
  createdAt: string;         // Creation timestamp
}

export interface ChunkedUploadCompleteResponse {
  fileId: string;
  projectId: string;
  folderId: string | null;
  name: string;
  url: string;
  createdAt: string;
}

export interface ChunkInfo {
  chunkIndex: number;
  chunkSize: number;
  start: number;
  end: number;
  uploadId: string;
}

class ChunkedUploadService {
  private static readonly DEFAULT_CHUNK_SIZE = 5 * 1024 * 1024; // 5MB
  private static readonly LARGE_FILE_THRESHOLD = 10 * 1024 * 1024; // 10MB
  private static readonly MAX_FILE_SIZE = 500 * 1024 * 1024; // 500MB


  private activeUploads = new Map<string, {
    file: File;
    options: ChunkUploadOptions;
    progress: ChunkUploadProgress;
    startTime: number;
    uploadedChunks: Set<number>;
    abortController: AbortController;
  }>();

  /**
   * Check if file should use chunked upload
   */
  shouldUseChunkedUpload(file: File): boolean {
    return file.size > ChunkedUploadService.LARGE_FILE_THRESHOLD;
  }

  /**
   * Validate file size
   */
  validateFileSize(file: File): { valid: boolean; error?: string } {
    if (file.size > ChunkedUploadService.MAX_FILE_SIZE) {
      return {
        valid: false,
        error: `File size ${this.formatBytes(file.size)} exceeds maximum allowed size of ${this.formatBytes(ChunkedUploadService.MAX_FILE_SIZE)}`
      };
    }
    return { valid: true };
  }

  /**
   * Start chunked upload using backend contract: init -> upload chunks -> complete
   */
  async uploadFile(
    file: File,
    destination: { type: 'project' | 'folder'; id: string; projectId?: string },
    options: ChunkUploadOptions = {},
    batchId?: string
  ): Promise<string> {
    // Validate file size
    const validation = this.validateFileSize(file);
    if (!validation.valid) {
      throw new Error(validation.error);
    }

    const fileId = this.generateFileId(file);
    const chunkSize = options.chunkSize || ChunkedUploadService.DEFAULT_CHUNK_SIZE;
    const total_chunks = Math.ceil(file.size / chunkSize);

    console.log(`ðŸ“¦ Starting chunked upload for ${file.name} (${this.formatBytes(file.size)}) in ${total_chunks} chunks of ${this.formatBytes(chunkSize)} each`);

    // Initialize progress tracking
    const progress: ChunkUploadProgress = {
      fileId,
      fileName: file.name,
      fileSize: file.size,
      uploadedBytes: 0,
      totalBytes: file.size,
      percentage: 0,
      uploadSpeed: 0,
      estimatedTimeRemaining: 0,
      currentChunk: 0,
      total_chunks,
      status: 'initializing'
    };

    const abortController = new AbortController();

    // Store upload info for progress tracking
    this.activeUploads.set(fileId, {
      file,
      options,
      progress,
      startTime: Date.now(),
      uploadedChunks: new Set(),
      abortController
    });

    try {
      // Step 1: Initialize chunked upload with backend
      this.updateProgress(fileId, { status: 'initializing' });
      const initResponse = await this.initializeChunkedUpload(file, destination, chunkSize);
      console.log(`âœ… Chunked upload initialized:`, initResponse);

      // Step 2: Upload all chunks (with limited concurrency)
    this.updateProgress(fileId, { status: 'uploading' });
    await this.uploadAllChunks(fileId, initResponse.uploadId, destination);
    console.log(`âœ… All chunks uploaded for ${file.name}`);

    // Step 3: Complete the upload (pass file + total_chunks + batchId)
    const completeResponse = await this.completeChunkedUpload(
      initResponse.uploadId,
      destination,
      file,
      total_chunks,
      batchId
    );
    console.log(`âœ… Chunked upload completed:`, completeResponse);


      // Update final progress
      this.updateProgress(fileId, { 
        status: 'completed', 
        uploadedBytes: file.size,
        percentage: 100 
      });

      return completeResponse.fileId;

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Chunked upload failed';
      console.error(`âŒ Chunked upload failed for ${file.name}:`, errorMsg);
      
      this.updateProgress(fileId, { 
        status: 'error', 
        errorMessage: errorMsg 
      });

      // Propagate error so useApiFileUpload can trigger fallback
      throw new Error(`Chunked upload failed: ${errorMsg}`);
    } finally {
      // Clean up after upload (success or failure)
      setTimeout(() => this.activeUploads.delete(fileId), 5000);
    }
  }





  /**
   * Initialize chunked upload with backend API
   * POST /api/projects/{id}/files/chunked/init or /api/folders/{id}/files/chunked/init
   */
  private async initializeChunkedUpload(
    file: File,
    destination: { type: 'project' | 'folder'; id: string; projectId?: string }, 
    chunkSize: number
  ): Promise<ChunkedUploadInitResponse> {
    const url = destination.type === 'project' 
      ? `http://localhost:8000/api/projects/${destination.id}/files/chunked/init`
      : `http://localhost:8000/api/folders/${destination.id}/files/chunked/init`;

    // Include relativePath if available (for folder uploads with subfolders)
    const relativePath = 'webkitRelativePath' in file ? (file as File & { webkitRelativePath: string }).webkitRelativePath : '';
    
    const requestBody: ChunkedUploadInitRequest = {
      fileName: file.name,
      fileSize: file.size,
      fileType: file.type,
      chunkSize: chunkSize,
      relativePath: relativePath || null
    };

    console.log(`ï¿½ Initializing chunked upload:`, { url, requestBody });

    const response = await fetch(url, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(requestBody)
    });

    if (!response.ok) {
      throw new Error(`Failed to initialize chunked upload: ${response.statusText}`);
    }

    const result = await response.json() as ChunkedUploadInitResponse;
    return result;
  }

  /**
   * Upload all chunks with limited concurrency
   */
  private async uploadAllChunks(
    fileId: string, 
    uploadId: string, 
    destination: { type: 'project' | 'folder'; id: string; projectId?: string }
  ): Promise<void> {
    const uploadInfo = this.activeUploads.get(fileId);
    if (!uploadInfo) throw new Error('Upload info not found');

    const { file, options } = uploadInfo;
    const chunkSize = options.chunkSize || ChunkedUploadService.DEFAULT_CHUNK_SIZE;
    const total_chunks = Math.ceil(file.size / chunkSize);

    // Create chunk info array
    const chunks: ChunkInfo[] = [];
    for (let i = 0; i < total_chunks; i++) {
      const start = i * chunkSize;
      const end = Math.min(start + chunkSize, file.size);
      chunks.push({
        chunkIndex: i,
        chunkSize: end - start,
        start,
        end,
        uploadId
      });
    }

    // Upload chunks with limited concurrency (3 at a time)
    const CONCURRENCY_LIMIT = 3;
    for (let i = 0; i < chunks.length; i += CONCURRENCY_LIMIT) {
      const chunkBatch = chunks.slice(i, i + CONCURRENCY_LIMIT);
      const batchPromises = chunkBatch.map(chunk => 
        this.uploadSingleChunk(fileId, chunk, destination)
      );
      
      await Promise.all(batchPromises);
      
      // Call onChunkComplete callback for each completed chunk in this batch
      chunkBatch.forEach(chunk => {
        if (uploadInfo.options.onChunkComplete) {
          uploadInfo.options.onChunkComplete(chunk.chunkIndex, total_chunks);
        }
      });
    }
  }

  /**
   * Upload a single chunk (updated for backend contract)
   */
  private async uploadSingleChunk(
    fileId: string, 
    chunk: ChunkInfo, 
    destination: { type: 'project' | 'folder'; id: string; projectId?: string }
  ): Promise<void> {
    const uploadInfo = this.activeUploads.get(fileId);
    if (!uploadInfo) throw new Error('Upload info not found');

    const { file } = uploadInfo;
    const chunkData = file.slice(chunk.start, chunk.end);
    
    const formData = new FormData();
    formData.append('chunk', chunkData);
    formData.append('chunkIndex', chunk.chunkIndex.toString());
    formData.append('uploadId', chunk.uploadId);

    const url = destination.type === 'project' 
      ? `http://localhost:8000/api/projects/${destination.id}/files/chunked/upload`
      : `http://localhost:8000/api/folders/${destination.id}/files/chunked/upload`;

    console.log(`ðŸ“¤ Uploading chunk ${chunk.chunkIndex + 1} (${this.formatBytes(chunk.chunkSize)})`);

    const response = await fetch(url, {
      method: 'POST',
      body: formData,
      signal: uploadInfo.abortController.signal
    });

    if (!response.ok) {
      throw new Error(`Chunk ${chunk.chunkIndex} upload failed: ${response.statusText}`);
    }

    // Update progress tracking
    uploadInfo.uploadedChunks.add(chunk.chunkIndex);
    const uploadedChunks = uploadInfo.uploadedChunks.size;

    const uploadedBytes = uploadedChunks * (uploadInfo.options.chunkSize || ChunkedUploadService.DEFAULT_CHUNK_SIZE);
    const actualUploadedBytes = Math.min(uploadedBytes, file.size);
    
    const elapsedTime = (Date.now() - uploadInfo.startTime) / 1000;
    const uploadSpeed = actualUploadedBytes / elapsedTime;
    const remainingBytes = file.size - actualUploadedBytes;
    const estimatedTimeRemaining = uploadSpeed > 0 ? remainingBytes / uploadSpeed : 0;

    this.updateProgress(fileId, {
      uploadedBytes: actualUploadedBytes,
      percentage: Math.round((actualUploadedBytes / file.size) * 100),
      uploadSpeed,
      estimatedTimeRemaining,
      currentChunk: uploadedChunks
    });

    console.log(`âœ… Chunk ${chunk.chunkIndex + 1} uploaded successfully`);
  }



  /**
   * Complete chunked upload using backend contract
   * POST /api/projects/{id}/files/chunked/complete or /api/folders/{id}/files/chunked/complete
   */
 private async completeChunkedUpload(
  uploadId: string,
  destination: { type: 'project' | 'folder'; id: string; projectId?: string },
  file: File,
  _total_chunks: number,
  batchId?: string
): Promise<ChunkedUploadCompleteResponse> {
  const url = destination.type === 'project'
    ? `http://localhost:8000/api/projects/${destination.id}/files/chunked/complete`
    : `http://localhost:8000/api/folders/${destination.id}/files/chunked/complete`;

  // Determine the correct project ID
  const projectId = destination.type === 'project' ? destination.id : (destination.projectId || destination.id);

  // ALWAYS ensure batch_id is present for chunked uploads
  const effectiveBatchId = batchId || crypto.randomUUID();

  // Create the request body that matches backend FrontendChunkedUploadCompleteRequest schema
  // Note: Backend only uses fileId (mapped to uploadId), but schema requires all fields
  const requestBody: ChunkedUploadCompleteRequest = {
    fileId: uploadId,  // This is the actual uploadId that backend uses
    projectId: projectId, // Correct project ID for schema validation
    folderId: destination.type === 'folder' ? destination.id : null,
    name: file.name,   // Schema requirement, backend will use session data
    url: 'temp-url',   // Schema requirement, backend will generate actual URL
    batch_id: effectiveBatchId, // ALWAYS include batch_id for chunked uploads
    createdAt: new Date().toISOString() // Schema requirement
  };

  console.log(`ðŸ”„ Completing chunked upload:`, { url, requestBody });

  const response = await fetch(url, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(requestBody),
  });

  if (!response.ok) {
    throw new Error(`Failed to complete chunked upload: ${response.statusText}`);
  }

  const result = (await response.json()) as ChunkedUploadCompleteResponse;
  return result;
}

  /**
   * Cancel upload
   */
  cancelUpload(fileId: string): void {
    const uploadInfo = this.activeUploads.get(fileId);
    if (uploadInfo) {
      uploadInfo.abortController.abort();
      this.updateProgress(fileId, { status: 'cancelled' });
    }
  }

  /**
   * Pause upload (for future implementation)
   */
  pauseUpload(fileId: string): void {
    const uploadInfo = this.activeUploads.get(fileId);
    if (uploadInfo) {
      this.updateProgress(fileId, { status: 'paused' });
    }
  }

  /**
   * Get upload progress
   */
  getProgress(fileId: string): ChunkUploadProgress | null {
    return this.activeUploads.get(fileId)?.progress || null;
  }

  /**
   * Get all active uploads
   */
  getActiveUploads(): ChunkUploadProgress[] {
    return Array.from(this.activeUploads.values()).map(info => info.progress);
  }

  /**
   * Update progress and notify
   */
  private updateProgress(fileId: string, updates: Partial<ChunkUploadProgress>): void {
    const uploadInfo = this.activeUploads.get(fileId);
    if (uploadInfo) {
      uploadInfo.progress = { ...uploadInfo.progress, ...updates };
      
      if (uploadInfo.options.onProgress) {
        uploadInfo.options.onProgress(uploadInfo.progress);
      }
    }
  }

  /**
   * Generate unique file ID
   */
  private generateFileId(file: File): string {
    return `${Date.now()}-${file.name}-${file.size}`;
  }

  /**
   * Format bytes to human readable
   */
  formatBytes(bytes: number): string {
    if (bytes === 0) return '0 Bytes';
    const k = 1024;
    const sizes = ['Bytes', 'KB', 'MB', 'GB'];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    return parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + ' ' + sizes[i];
  }

  /**
   * Format time duration (for future use with ETA display)
   */
  formatTime(seconds: number): string {
    if (seconds === 0 || !isFinite(seconds)) return '';
    if (seconds < 60) return `${Math.round(seconds)}s`;
    if (seconds < 3600) return `${Math.round(seconds / 60)}m ${Math.round(seconds % 60)}s`;
    return `${Math.round(seconds / 3600)}h ${Math.round((seconds % 3600) / 60)}m`;
  }
}

// Export singleton instance
export const chunkedUploadService = new ChunkedUploadService();
